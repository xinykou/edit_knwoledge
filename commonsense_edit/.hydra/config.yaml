editor_type: lora-hyperdecoders-postfusion-postfusion
train_type: m-cb+cf
test_type: m-test_cf
checkpoint_filename: randomlora-hyperdecoders-postfusion-m-cb+cf_t5-large-epoch=04-val_loss=0.0028.ckpt
train_mode: train
random_evidence: false
noisy_evidence: false
random_add_noisy_evidence: true
num_aug_sources: 2
experiment:
  task: QA
  dataset: commonsense
  s-un_cb: /media/data/1/yx/code/edit_knowledge_patch/commonsense_edit/dataset/my_created/s-un_cb/pretrain.csv
  s-cf: /media/data/1/yx/code/edit_knowledge_patch/commonsense_edit/dataset/my_created/s-cf/train.csv
  m-cb+cf: /media/data/1/yx/code/edit_knowledge_patch/commonsense_edit/dataset/my_created/m-cb+cf/train.csv
  m-cb+cf+a: /media/data/1/yx/code/edit_knowledge_patch/commonsense_edit/dataset/my_created/m-cb+cf+a/train.csv
  s-test_cb: /media/data/1/yx/code/edit_knowledge_patch/commonsense_edit/dataset/my_created/s-test_cb/test.csv
  s-test_cf: /media/data/1/yx/code/edit_knowledge_patch/commonsense_edit/dataset/my_created/s-test_cf/test.csv
  m-test_cf: /media/data/1/yx/code/edit_knowledge_patch/commonsense_edit/dataset/my_created/m-test_cf/test.csv
  m-test_cb: /media/data/1/yx/code/edit_knowledge_patch/commonsense_edit/dataset/my_created/m-test_cb/test.csv
model:
  model_name: t5-large
  model_class: T5ForConditionalGenerationWithAdapterWithFusion
  tokenizer_class: T5Tokenizer
  config_class: T5WithAdapterConfig
  model_cache: /media/data/1/yx/data/model_cache/t5-large
  fine_tune:
    checkpoints_dirpath: /media/data/1/yx/code/edit_knowledge_patch/commonsense_edit/trained_model
    batch_size: 8
    learning_rate: 0.001
    n_epochs: 5
    source_max_token_len: 56
    target_max_token_len: 20
    context_max_token_len: 56
  model_inference:
    results_dir: /media/data/1/yx/code/edit_knowledge_patch/commonsense_edit/dataset/predict_result
    input_max_length: 56
    output_max_length: 20
    context_max_token_len: 56
    repetition_penalty: 2.5
    length_penalty: 1.0
    num_beams: 1
    batch_size: 16
editor:
  _name: lora_hyperdecoders_postfusion
  encoder_adapter: manual
  decoder_adapter: generated
  encoder_adapter_dim: 64
  decoder_adapter_dim: 64
  adapter_norm_input: true
  freeze_model: true
  unfreeze_encoder_adapters: true
  unfreeze_decoder_adapters: true
  freeze_decoder_adapters_down: false
  freeze_decoder_adapters_up: false
  process_encoder_output: true
  dim: 1024
  depth: 1
  dim_head: 64
  heads: 8
  num_latents: 64
  num_aug_sources: 2
  ff_mult: 2
  xattn_ff_mult: 4
  freeze_lm: false
  cross_attn_every: 1
  only_attend_immediate_media: false
  num_xattn_layers: 1
  unfreeze_gated_and_perceiver: true
