_name: lora_hyperdecoders_postfusion_vip

# HyperNet adapter 参数
encoder_adapter: manual      # 设置为None后， 不添加adapter
decoder_adapter: generated   # 设置 这个参数 不等于 generated 后 就不会用超网络生成 ”adapters“ 的矩阵； 设置为None 后不添加 adapter
encoder_adapter_dim: 64  # 维度下降后 所对应的 维度
decoder_adapter_dim: 64
adapter_norm_input: true
freeze_model: true
unfreeze_encoder_adapters: true
unfreeze_decoder_adapters: true
process_encoder_output: true # "Whether to pass the encoder output through a MLP before mean-pooling or not."

# Context + Question 融合时的 交叉注意力参数
dim: 1024      # Perceiver Resamplerz中，每个 Latent Query 的维度是 dim=1024
depth: 1
dim_head: 64  # 每个head 的维度
heads: 8
num_latents: 64    # Perceiver Resampler中， context单独注意力时 Latent Query是可学习的，这里设定的 64个可以学习的向量，
num_aug_sources: 2  # Perceiver Resampler中， 这个代表每个样本的 context数量，用于设定
ff_mult: 2     # Perceiver Resampler中，线性层维度放大的倍数
xattn_ff_mult: 4  # gated cross attention中，线性层维度放大的倍数
freeze_lm: false
cross_attn_every: 1  # gated cross attention中 cross attention的数量
only_attend_immediate_media: false
num_xattn_layers: 1   # gated cross attention, 也就是 question和 contexts互注意力的层数
unfreeze_gated_and_perceiver: True  #


# vip模块参数
num_cq_tokens: 100  # prompt length
codebook_size: 1000 # prompt length * 10
num_codes: 1000
project_dim: 64  # project newtwork 的 hidden_dim
trans_attention_head: 4
trans_linear_dim: 256  # hidden_dim * 2
trans_num_layers: 2
temperature: 100 # 温度系数
num_samples: 10
commitment_cost: 0.1
_decay: 0.99
_epsilon: 1e-2
unfreeze_vip: True







